{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to work with Fundamental data\n",
    "\n",
    "The Securities and Exchange Commission (SEC) requires US issuers, that is, listed companies and securities, including mutual funds to file three quarterly financial statements (Form 10-Q) and one annual report (Form 10-K), in addition to various other regulatory filing requirements.\n",
    "\n",
    "Since the early 1990s, the SEC made these filings available through its Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system. They constitute the primary data source for the fundamental analysis of equity and other securities, such as corporate credit, where the value depends on the business prospects and financial health of the issuer. \n",
    "\n",
    "#### Automated processing using XBRL markup\n",
    "\n",
    "Automated analysis of regulatory filings has become much easier since the SEC introduced XBRL, a free, open, and global standard for the electronic representation and exchange of business reports. XBRL is based on XML; it relies on [taxonomies](https://www.sec.gov/dera/data/edgar-log-file-data-set.html) that define the meaning of the elements of a report and map to tags that highlight the corresponding information in the electronic version of the report. One such taxonomy represents the US Generally Accepted Accounting Principles (GAAP).\n",
    "\n",
    "The SEC introduced voluntary XBRL filings in 2005 in response to accounting scandals before requiring this format for all filers since 2009 and continues to expand the mandatory coverage to other regulatory filings. The SEC maintains a website that lists the current taxonomies that shape the content of different filings and can be used to extract specific items.\n",
    "\n",
    "There are several avenues to track and access fundamental data reported to the SEC:\n",
    "- As part of the [EDGAR Public Dissemination Service]((https://www.sec.gov/oit/announcement/public-dissemination-service-system-contact.html)) (PDS), electronic feeds of accepted filings are available for a fee. \n",
    "- The SEC updates [RSS feeds](https://www.sec.gov/structureddata/rss-feeds-submitted-filings) every 10 minutes, which list structured disclosure submissions.\n",
    "- There are public [index files](https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm) for the retrieval of all filings through FTP for automated processing.\n",
    "- The financial statement (and notes) datasets contain parsed XBRL data from all financial statements and the accompanying notes.\n",
    "\n",
    "The SEC also publishes log files containing the [internet search traffic](https://www.sec.gov/dera/data/edgar-log-file-data-set.html) for EDGAR filings through SEC.gov, albeit with a six-month delay.\n",
    "\n",
    "\n",
    "#### Building a fundamental data time series\n",
    "\n",
    "The scope of the data in the [Financial Statement and Notes](https://www.sec.gov/dera/data/financial-statement-and-notes-data-set.html) datasets consists of numeric data extracted from the primary financial statements (Balance sheet, income statement, cash flows, changes in equity, and comprehensive income) and footnotes on those statements. The data is available as early as 2009.\n",
    "\n",
    "\n",
    "The folder [03_sec_edgar](03_sec_edgar) contains the notebook [edgar_xbrl](03_sec_edgar/edgar_xbrl.ipynb) to download and parse EDGAR data in XBRL format, and create fundamental metrics like the P/E ratio by combining financial statement and price data.\n",
    "\n",
    "### Other fundamental data sources\n",
    "\n",
    "- [Compilation of macro resources by the Yale Law School](https://library.law.yale.edu/news/75-sources-economic-data-statistics-reports-and-commentary)\n",
    "- [Capital IQ](www.capitaliq.com)\n",
    "- [Compustat](www.compustat.com)\n",
    "- [MSCI Barra](www.mscibarra.com)\n",
    "- [Northfield Information Services](www.northinfo.com)\n",
    "- [Quantitative Services Group](www.qsg.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with filing data from the SEC's EDGAR service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "import json\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "import requests\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid') # figure aesthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data') # perhaps set to external harddrive to coomodate large amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEC_URL = 'https://www.sec.gov/'\n",
    "FSN_PATH = 'files/dera/data/financial-statement-and-notes-data-sets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "this_year = today.year\n",
    "this_quarter = today.quarter\n",
    "past_years = range(2014, this_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2019, 3), (2019, 4), (2020, 1), (2020, 2), (2020, 3), (2020, 4), (2021, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add previous quarters\n",
    "filing_periods = [(y, q) for y in past_years for q in range(1, 5)]\n",
    "# Add this year's quarters\n",
    "filing_periods.extend([(this_year, q) for q in range(1, this_quarter + 1)])\n",
    "filing_periods[-7:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_filing_data(filing_periods):\n",
    "    for i, (yr, qtr) in enumerate(filing_periods, 1):\n",
    "        print(f'{yr}-Q{qtr}', end=' ', flush=True)\n",
    "        filing = f'{yr}q{qtr}_notes.zip'\n",
    "        path = data_path / f'{yr}_{qtr}' / 'source'\n",
    "        if not path.exists():\n",
    "            path.mkdir(exist_ok=True, parents=True)\n",
    "        url = SEC_URL + FSN_PATH + filing\n",
    "        print('url = ' + url)\n",
    "        # 2020q1 is currently (Oct 2020) in a different location; this may change at some point\n",
    "#         if yr == 2020 and qtr == 1:\n",
    "#             url = SEC_URL + 'files/node/add/data_distribution/' + filing\n",
    "\n",
    "        response = requests.get(url).content\n",
    "        try:\n",
    "            with ZipFile(BytesIO(response)) as zip_file:\n",
    "                for file in zip_file.namelist():\n",
    "                    local_file = path / file\n",
    "                    if local_file.exists():\n",
    "                        continue\n",
    "                    with local_file.open('wb') as output:\n",
    "                        for line in zip_file.open(file).readlines():\n",
    "                            output.write(line)\n",
    "        except BadZipFile as e:\n",
    "            print(e)\n",
    "            print('got bad zip file')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-Q3 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2019q3_notes.zip\n",
      "2019-Q4 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2019q4_notes.zip\n",
      "2020-Q1 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2020q1_notes.zip\n",
      "2020-Q2 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2020q2_notes.zip\n",
      "2020-Q3 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2020q3_notes.zip\n",
      "2020-Q4 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2020q4_notes.zip\n",
      "2021-Q1 url = https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2021q1_notes.zip\n",
      "File is not a zip file\n",
      "got bad zip file\n"
     ]
    }
   ],
   "source": [
    "fetch_filing_data(filing_periods[-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "AWS_REGION = 'us-west-2'\n",
    "FUNDAMENTAL_DATA_PATH = 'fundamentals-data'\n",
    "AWS_PROFILE_NAME = 'gold-miner'\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            session = boto3.Session(profile_name='gold-miner')\n",
    "            s3_client = session.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            session = boto3.Session(profile_name='gold-miner', region_name='us-west-2')\n",
    "            s3_client = session.client('s3')\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# create_bucket(FUNDAMENTAL_DATA_PATH, AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  fundamentals-data\n"
     ]
    }
   ],
   "source": [
    "# List all the existing buckets for the AWS account.\n",
    "session = boto3.Session(profile_name='gold-miner', region_name='us-west-2')\n",
    "s3_client = session.client('s3')\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./fundamental_data_edgar_xbrl.ipynb\n",
      "./README.md\n",
      "./.ipynb_checkpoints/fundamental_data_edgar_xbrl-checkpoint.ipynb\n",
      "./data/2015_1/source/ren.tsv\n",
      "./data/2015_1/source/pre.tsv\n",
      "./data/2015_1/source/sub.tsv\n",
      "./data/2015_1/source/txt.tsv\n",
      "./data/2015_1/source/cal.tsv\n",
      "./data/2015_1/source/tag.tsv\n",
      "./data/2015_1/source/readme.htm\n",
      "./data/2015_1/source/2015q1_notes-metadata.json\n",
      "./data/2015_1/source/dim.tsv\n",
      "./data/2015_1/source/num.tsv\n",
      "./data/2017_2/source/ren.tsv\n",
      "./data/2017_2/source/pre.tsv\n",
      "./data/2017_2/source/sub.tsv\n",
      "./data/2017_2/source/txt.tsv\n",
      "./data/2017_2/source/cal.tsv\n",
      "./data/2017_2/source/tag.tsv\n",
      "./data/2017_2/source/readme.htm\n",
      "./data/2017_2/source/dim.tsv\n",
      "./data/2017_2/source/2017q2_notes-metadata.json\n",
      "./data/2017_2/source/num.tsv\n",
      "./data/2019_3/source/ren.tsv\n",
      "./data/2019_3/source/pre.tsv\n",
      "./data/2019_3/source/sub.tsv\n",
      "./data/2019_3/source/cal.tsv\n",
      "./data/2019_3/source/tag.tsv\n",
      "./data/2019_3/source/dim.tsv\n",
      "./data/2017_3/source/ren.tsv\n",
      "./data/2017_3/source/pre.tsv\n",
      "./data/2017_3/source/sub.tsv\n",
      "./data/2017_3/source/txt.tsv\n",
      "./data/2017_3/source/cal.tsv\n",
      "./data/2017_3/source/tag.tsv\n",
      "./data/2017_3/source/readme.htm\n",
      "./data/2017_3/source/dim.tsv\n",
      "./data/2017_3/source/2017q3_notes-metadata.json\n",
      "./data/2017_3/source/num.tsv\n",
      "./data/2017_4/source/ren.tsv\n",
      "./data/2017_4/source/pre.tsv\n",
      "./data/2017_4/source/sub.tsv\n",
      "./data/2017_4/source/txt.tsv\n",
      "./data/2017_4/source/cal.tsv\n",
      "./data/2017_4/source/tag.tsv\n",
      "./data/2017_4/source/readme.htm\n",
      "./data/2017_4/source/2017q4_notes-metadata.json\n",
      "./data/2017_4/source/dim.tsv\n",
      "./data/2017_4/source/num.tsv\n",
      "./data/2019_2/source/ren.tsv\n",
      "./data/2019_2/source/pre.tsv\n",
      "./data/2019_2/source/sub.tsv\n",
      "./data/2019_2/source/txt.tsv\n",
      "./data/2019_2/source/cal.tsv\n",
      "./data/2019_2/source/tag.tsv\n",
      "./data/2019_2/source/readme.htm\n",
      "./data/2019_2/source/dim.tsv\n",
      "./data/2019_2/source/num.tsv\n",
      "./data/2019_2/source/2019q2_notes-metadata.json\n",
      "./data/2020_1/source/ren.tsv\n",
      "./data/2020_1/source/pre.tsv\n",
      "./data/2020_1/source/sub.tsv\n",
      "./data/2020_1/source/txt.tsv\n",
      "./data/2020_1/source/cal.tsv\n",
      "./data/2020_1/source/tag.tsv\n",
      "./data/2020_1/source/dim.tsv\n",
      "./data/2020_1/source/num.tsv\n",
      "./data/2016_1/source/ren.tsv\n",
      "./data/2016_1/source/pre.tsv\n",
      "./data/2016_1/source/sub.tsv\n",
      "./data/2016_1/source/2016q1_notes-metadata.json\n",
      "./data/2016_1/source/txt.tsv\n",
      "./data/2016_1/source/cal.tsv\n",
      "./data/2016_1/source/tag.tsv\n",
      "./data/2016_1/source/readme.htm\n",
      "./data/2016_1/source/dim.tsv\n",
      "./data/2016_1/source/num.tsv\n",
      "./data/2014_2/source/ren.tsv\n",
      "./data/2014_2/source/pre.tsv\n",
      "./data/2014_2/source/2014q2_notes-metadata.json\n",
      "./data/2014_2/source/sub.tsv\n",
      "./data/2014_2/source/txt.tsv\n",
      "./data/2014_2/source/cal.tsv\n",
      "./data/2014_2/source/tag.tsv\n",
      "./data/2014_2/source/readme.htm\n",
      "./data/2014_2/source/dim.tsv\n",
      "./data/2014_2/source/num.tsv\n",
      "./data/2018_1/source/ren.tsv\n",
      "./data/2018_1/source/pre.tsv\n",
      "./data/2018_1/source/sub.tsv\n",
      "./data/2018_1/source/2018q1_notes-metadata.json\n",
      "./data/2018_1/source/txt.tsv\n",
      "./data/2018_1/source/cal.tsv\n",
      "./data/2018_1/source/tag.tsv\n",
      "./data/2018_1/source/readme.htm\n",
      "./data/2018_1/source/dim.tsv\n",
      "./data/2018_1/source/num.tsv\n",
      "./data/2014_3/source/ren.tsv\n",
      "./data/2014_3/source/2014q3_notes-metadata.json\n",
      "./data/2014_3/source/pre.tsv\n",
      "./data/2014_3/source/sub.tsv\n",
      "./data/2014_3/source/txt.tsv\n",
      "./data/2014_3/source/cal.tsv\n",
      "./data/2014_3/source/tag.tsv\n",
      "./data/2014_3/source/readme.htm\n",
      "./data/2014_3/source/dim.tsv\n",
      "./data/2014_3/source/num.tsv\n",
      "./data/2014_4/source/ren.tsv\n",
      "./data/2014_4/source/pre.tsv\n",
      "./data/2014_4/source/sub.tsv\n",
      "./data/2014_4/source/2014q4_notes-metadata.json\n",
      "./data/2014_4/source/txt.tsv\n",
      "./data/2014_4/source/cal.tsv\n",
      "./data/2014_4/source/tag.tsv\n",
      "./data/2014_4/source/readme.htm\n",
      "./data/2014_4/source/dim.tsv\n",
      "./data/2014_4/source/num.tsv\n",
      "./data/2015_2/source/ren.tsv\n",
      "./data/2015_2/source/pre.tsv\n",
      "./data/2015_2/source/sub.tsv\n",
      "./data/2015_2/source/txt.tsv\n",
      "./data/2015_2/source/cal.tsv\n",
      "./data/2015_2/source/tag.tsv\n",
      "./data/2015_2/source/readme.htm\n",
      "./data/2015_2/source/2015q2_notes-metadata.json\n",
      "./data/2015_2/source/dim.tsv\n",
      "./data/2015_2/source/num.tsv\n",
      "./data/2017_1/source/ren.tsv\n",
      "./data/2017_1/source/pre.tsv\n",
      "./data/2017_1/source/sub.tsv\n",
      "./data/2017_1/source/txt.tsv\n",
      "./data/2017_1/source/cal.tsv\n",
      "./data/2017_1/source/tag.tsv\n",
      "./data/2017_1/source/readme.htm\n",
      "./data/2017_1/source/2017q1_notes-metadata.json\n",
      "./data/2017_1/source/dim.tsv\n",
      "./data/2017_1/source/num.tsv\n",
      "./data/2019_1/source/ren.tsv\n",
      "./data/2019_1/source/pre.tsv\n",
      "./data/2019_1/source/sub.tsv\n",
      "./data/2019_1/source/txt.tsv\n",
      "./data/2019_1/source/cal.tsv\n",
      "./data/2019_1/source/tag.tsv\n",
      "./data/2019_1/source/readme.htm\n",
      "./data/2019_1/source/dim.tsv\n",
      "./data/2019_1/source/2019q1_notes-metadata.json\n",
      "./data/2019_1/source/num.tsv\n",
      "./data/2015_4/source/ren.tsv\n",
      "./data/2015_4/source/pre.tsv\n",
      "./data/2015_4/source/sub.tsv\n",
      "./data/2015_4/source/txt.tsv\n",
      "./data/2015_4/source/cal.tsv\n",
      "./data/2015_4/source/tag.tsv\n",
      "./data/2015_4/source/readme.htm\n",
      "./data/2015_4/source/2015q4_notes-metadata.json\n",
      "./data/2015_4/source/dim.tsv\n",
      "./data/2015_4/source/num.tsv\n",
      "./data/2015_3/source/ren.tsv\n",
      "./data/2015_3/source/pre.tsv\n",
      "./data/2015_3/source/sub.tsv\n",
      "./data/2015_3/source/txt.tsv\n",
      "./data/2015_3/source/cal.tsv\n",
      "./data/2015_3/source/tag.tsv\n",
      "./data/2015_3/source/readme.htm\n",
      "./data/2015_3/source/2015q3_notes-metadata.json\n",
      "./data/2015_3/source/dim.tsv\n",
      "./data/2015_3/source/num.tsv\n",
      "./data/2020_3/source/ren.tsv\n",
      "./data/2020_3/source/pre.tsv\n",
      "./data/2020_3/source/sub.tsv\n",
      "./data/2020_3/source/txt.tsv\n",
      "./data/2020_3/source/cal.tsv\n",
      "./data/2020_3/source/2020q3_notes-metadata.json\n",
      "./data/2020_3/source/tag.tsv\n",
      "./data/2020_3/source/readme.htm\n",
      "./data/2020_3/source/dim.tsv\n",
      "./data/2020_3/source/num.tsv\n",
      "./data/2020_4/source/sub.tsv\n",
      "./data/2020_4/source/tag.tsv\n",
      "./data/2020_4/source/dim.tsv\n",
      "./data/2020_2/source/ren.tsv\n",
      "./data/2020_2/source/pre.tsv\n",
      "./data/2020_2/source/sub.tsv\n",
      "./data/2020_2/source/txt.tsv\n",
      "./data/2020_2/source/cal.tsv\n",
      "./data/2020_2/source/2020q2_notes-metadata.json\n",
      "./data/2020_2/source/tag.tsv\n",
      "./data/2020_2/source/readme.htm\n",
      "./data/2020_2/source/dim.tsv\n",
      "./data/2020_2/source/num.tsv\n",
      "./data/2016_2/source/ren.tsv\n",
      "./data/2016_2/source/pre.tsv\n",
      "./data/2016_2/source/sub.tsv\n",
      "./data/2016_2/source/2016q2_notes-metadata.json\n",
      "./data/2016_2/source/txt.tsv\n",
      "./data/2016_2/source/cal.tsv\n",
      "./data/2016_2/source/tag.tsv\n",
      "./data/2016_2/source/readme.htm\n",
      "./data/2016_2/source/dim.tsv\n",
      "./data/2016_2/source/num.tsv\n",
      "./data/2014_1/source/ren.tsv\n",
      "./data/2014_1/source/pre.tsv\n",
      "./data/2014_1/source/sub.tsv\n",
      "./data/2014_1/source/2014q1_notes-metadata.json\n",
      "./data/2014_1/source/txt.tsv\n",
      "./data/2014_1/source/cal.tsv\n",
      "./data/2014_1/source/tag.tsv\n",
      "./data/2014_1/source/readme.htm\n",
      "./data/2014_1/source/dim.tsv\n",
      "./data/2014_1/source/num.tsv\n",
      "./data/2018_4/source/ren.tsv\n",
      "./data/2018_4/source/pre.tsv\n",
      "./data/2018_4/source/sub.tsv\n",
      "./data/2018_4/source/txt.tsv\n",
      "./data/2018_4/source/cal.tsv\n",
      "./data/2018_4/source/2018q4_notes-metadata.json\n",
      "./data/2018_4/source/tag.tsv\n",
      "./data/2018_4/source/readme.htm\n",
      "./data/2018_4/source/dim.tsv\n",
      "./data/2018_4/source/num.tsv\n",
      "./data/2018_3/source/ren.tsv\n",
      "./data/2018_3/source/pre.tsv\n",
      "./data/2018_3/source/sub.tsv\n",
      "./data/2018_3/source/2018q3_notes-metadata.json\n",
      "./data/2018_3/source/txt.tsv\n",
      "./data/2018_3/source/cal.tsv\n",
      "./data/2018_3/source/tag.tsv\n",
      "./data/2018_3/source/readme.htm\n",
      "./data/2018_3/source/dim.tsv\n",
      "./data/2018_3/source/num.tsv\n",
      "./data/2016_4/source/ren.tsv\n",
      "./data/2016_4/source/pre.tsv\n",
      "./data/2016_4/source/sub.tsv\n",
      "./data/2016_4/source/2016q4_notes-metadata.json\n",
      "./data/2016_4/source/txt.tsv\n",
      "./data/2016_4/source/cal.tsv\n",
      "./data/2016_4/source/tag.tsv\n",
      "./data/2016_4/source/readme.htm\n",
      "./data/2016_4/source/dim.tsv\n",
      "./data/2016_4/source/num.tsv\n",
      "./data/2016_3/source/ren.tsv\n",
      "./data/2016_3/source/pre.tsv\n",
      "./data/2016_3/source/sub.tsv\n",
      "./data/2016_3/source/2016q3_notes-metadata.json\n",
      "./data/2016_3/source/txt.tsv\n",
      "./data/2016_3/source/cal.tsv\n",
      "./data/2016_3/source/tag.tsv\n",
      "./data/2016_3/source/readme.htm\n",
      "./data/2016_3/source/dim.tsv\n",
      "./data/2016_3/source/num.tsv\n",
      "./data/2018_2/source/ren.tsv\n",
      "./data/2018_2/source/pre.tsv\n",
      "./data/2018_2/source/sub.tsv\n",
      "./data/2018_2/source/2018q2_notes-metadata.json\n",
      "./data/2018_2/source/txt.tsv\n",
      "./data/2018_2/source/cal.tsv\n",
      "./data/2018_2/source/tag.tsv\n",
      "./data/2018_2/source/readme.htm\n",
      "./data/2018_2/source/dim.tsv\n",
      "./data/2018_2/source/num.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# traverse root directory, and list directories as dirs and files as files\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "#     path = root.split(os.sep)\n",
    "#     print(root)\n",
    "#     print((len(path) - 1) * '---', os.path.basename(root))\n",
    "    for file in files:\n",
    "        file_path = root + \"/\" + file\n",
    "        print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_folder_to_s3(folder, bucket, s3_client):\n",
    "    for root, dirs, files in os.walk(\"./data\"):\n",
    "        for file in files:\n",
    "            file_path = root + \"/\" + file\n",
    "            upload_file_to_s3(file_path, bucket, file_path[2:], s3_client)\n",
    "            print(\"uploading file: \", file_path)\n",
    "        \n",
    "def upload_file_to_s3(file_name, bucket, key, s3_client):\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, key)\n",
    "        \n",
    "    except:\n",
    "        print(\"Error when uploading file: \" + file_name)\n",
    "    \n",
    "# upload_folder_to_s3('./data', FUNDAMENTAL_DATA_PATH, s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_upload_folder_to_s3(folder, bucket, s3_client):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for root, dirs, files in os.walk(\"./data\"):\n",
    "            for file in files:\n",
    "                file_path = root + \"/\" + file\n",
    "                print(\"uploading file: \", file_path)\n",
    "                upload_file_to_s3(file_path, bucket, file_path[2:], s3_client)\n",
    "                future = executor.submit(upload_file_to_s3, file_path, bucket, file_path[2:], s3_client)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading file:  ./data/2015_1/source/ren.tsv\n",
      "uploading file:  ./data/2015_1/source/pre.tsv\n",
      "uploading file:  ./data/2015_1/source/sub.tsv\n",
      "uploading file:  ./data/2015_1/source/txt.tsv\n",
      "Error when uploading file: ./data/2015_1/source/txt.tsv\n",
      "uploading file:  ./data/2015_1/source/cal.tsv\n"
     ]
    }
   ],
   "source": [
    "parallel_upload_folder_to_s3('./data', FUNDAMENTAL_DATA_PATH, s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gold-miner",
   "language": "python",
   "name": "gold-miner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
